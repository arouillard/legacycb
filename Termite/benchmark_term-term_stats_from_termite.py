# -*- coding: utf-8 -*-
"""
Andrew D. Rouillard
Computational Biologist
Target Sciences
GSK
andrew.d.rouillard@gsk.com
"""

import os
import argparse
import numpy as np
import pandas as pd
import datasetIO
from scipy.stats import hypergeom
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, precision_recall_curve, roc_curve
from sklearn.preprocessing import RobustScaler
from matplotlib import pyplot as plt

def pointentropy(subset_count, total_count):
    if subset_count == 0:
        return 0
    else:
        return (subset_count/total_count)*(np.log2(total_count) - np.log2(subset_count))

def mutualinformation(tp, fp, fn, tn):
    n = tp + fp + fn + tn
    Hr = pointentropy(tp+fp,n) + pointentropy(fn+tn,n)
    Hc = pointentropy(tp+fn,n) + pointentropy(fp+tn,n)
    Hrc = pointentropy(tp,n) + pointentropy(fp,n) + pointentropy(fn,n) + pointentropy(tn,n)
    mi = Hr + Hc - Hrc
    nmi = mi/np.sqrt(Hr*Hc)
    iqr = mi/Hrc
    return mi, nmi, iqr

def get_step_density(x, num_bins='auto', bounds='auto', apply_log10_transformation=False):
    if apply_log10_transformation:
        x = np.log10(x)
    if bounds == 'auto':
        try:
            densities, edges = np.histogram(x, num_bins, density=True)
        except MemoryError:
            densities, edges = np.histogram(x, 20, density=True)
    else:
        try:
            densities, edges = np.histogram(x, num_bins, bounds, density=True)
        except MemoryError:
            densities, edges = np.histogram(x, 20, bounds, density=True)
    densities = np.concatenate((np.append([0], densities).reshape(-1,1), np.append(densities, [0]).reshape(-1,1)), 1).reshape(-1)
    edges = np.concatenate((edges.reshape(-1,1), edges.reshape(-1,1)), 1).reshape(-1)
    return densities, edges

def plot_step_density(x, xlabel, title, save_path, num_bins='auto', bounds='auto', apply_log10_transformation=False, x_zoom_bounds=None):
    fg_pdf, ax_pdf = plt.subplots(1, 1, figsize=(3,2))
    if type(x) == dict:
        for label, (fmt, values) in x.items():
            densities, edges = get_step_density(values, num_bins, bounds, apply_log10_transformation)
            ax_pdf.plot(edges, densities, fmt, linewidth=1, label=label)
    else:
        densities, edges = get_step_density(x, num_bins, bounds, apply_log10_transformation)
        ax_pdf.plot(edges, densities, '-k', linewidth=0.5)
        ax_pdf.fill_between(edges, densities, linewidth=0, color='k', alpha=0.5)
    ax_pdf.set_position([0.55/3, 0.35/2, 2.1/3, 1.3/2]) # left, bottom, width, height
    ax_pdf.set_title(title, fontsize=8)
    ax_pdf.set_ylabel('Probability Density', fontsize=8, labelpad=4)
    ax_pdf.set_xlabel(xlabel, fontsize=8, labelpad=2)
    if x_zoom_bounds != None:
        ax_pdf.set_xlim(x_zoom_bounds)
    elif bounds != 'auto':
        ax_pdf.set_xlim(bounds)
    ax_pdf.legend(loc='best', ncol=1, fontsize=8, frameon=False, borderpad=1.5, labelspacing=0.1, handletextpad=0.1, borderaxespad=0)
    ax_pdf.tick_params(axis='both', which='major', bottom=True, top=False, left=True, right=False, labelbottom=True, labeltop=False, labelleft=True, labelright=False, labelsize=8)
    ax_pdf.ticklabel_format(axis='both', style='sci', scilimits=(-3,3), fontsize=8)
    ax_pdf.yaxis.offsetText.set_fontsize(8)
    ax_pdf.xaxis.offsetText.set_fontsize(8)
    fg_pdf.savefig(save_path, transparent=True, pad_inches=0, dpi=300)
    plt.close()
    return densities, edges

def main(dictionaries, year, datestamp, min_score, universe, n_prior, min_count, association_statistic, reference_datamatrix_path, save_predictions):
    
    print('begin benchmark_term-term_stats_from_termite.py')
    
    print('dictionaries: {0}, {1}'.format(dictionaries[0], dictionaries[1]))
    print('year: {0}'.format(year))
    print('datestamp: {0}'.format(datestamp))
    print('min_score: {0!s}'.format(min_score))
    print('universe: {0}'.format(universe))
    print('n_prior: {0!s}'.format(n_prior))
    print('min_count: {0!s}'.format(min_count))
    print('association_statistic: {0}'.format(association_statistic))
    print('reference_datamatrix_path: {0}'.format(reference_datamatrix_path))
    print('save_predictions: {0!s}'.format(save_predictions))
    
    # create figures folder
    print('creating figures folder...')
    figures_folder = 'benchmark_figures'
    if not os.path.exists(figures_folder):
        os.mkdir(figures_folder)
    
    # load counts datamatrix
    # this file is generated by count_term-term_pmids_from_termite.py    
    print('loading counts datamatrix...')
    row_dictionary = dictionaries[0] # 'HUCELL', 'ANAT', 'INDICATION', 'HUCELLANAT', 'HUCELLANATINDICATION'
    column_dictionary = dictionaries[1] # 'HUCELL', 'ANAT', 'INDICATION', 'HUCELLANAT', 'HUCELLANATINDICATION'
    counts_datamatrix_path = '{0}_{1}_datamatrix_pmidcounts_year_{2}_datestamp_{3}_minscore_{4!s}.pickle'.format(row_dictionary, column_dictionary, year, datestamp, min_score)
    term_term_counts_all = datasetIO.load_datamatrix(counts_datamatrix_path)
    print('counts_datamatrix_path: {0}'.format(counts_datamatrix_path))
    print(term_term_counts_all)
    
    # load association statistic datamatrix
    # this file is generated by calc_term-term_stats_from_termite.py    
    print('loading association statistic datamatrix...')
    stats_datamatrix_path = '{0}_{1}_datamatrix_{2}_yr_{3}_ds_{4}_ms_{5!s}_uv_{6}_np_{7!s}_mc_{8!s}.pickle'.format(row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count)
    term_term_stats_all = datasetIO.load_datamatrix(stats_datamatrix_path)
    print('stats_datamatrix_path: {0}'.format(stats_datamatrix_path))
    print(term_term_stats_all)
    
    # load reference datamatrix of positive and negative examples
    print('loading reference datamatrix of positive and negative examples...')
    term_term_ref = datasetIO.load_datamatrix(reference_datamatrix_path)
    print('reference_datamatrix_path: {0}'.format(reference_datamatrix_path))
    print(term_term_ref)
    
    # align datamatrices to reference
    print('aligning datamatrices to reference...')
    term_term_counts = term_term_counts_all.tolabels(rowlabels=term_term_ref.rowlabels.copy(), columnlabels = term_term_ref.columnlabels.copy())
    term_term_stats = term_term_stats_all.tolabels(rowlabels=term_term_ref.rowlabels.copy(), columnlabels = term_term_ref.columnlabels.copy())
    
    # find term-term pairs with sufficient counts
    print('finding term-term pairs with sufficient counts...')
    I, J = (term_term_counts.matrix >= min_count).nonzero()
    num_sufficient = I.size
    print('term-term pairs with at least {0!s} counts: {1!s}'.format(min_count, num_sufficient))
    
    # find row_term_dicts and column_term_dicts
    print('finding row_term_dicts and column_term_dicts')
    row_term_dicts = np.unique(term_term_stats.rowmeta['term_dict'])
    column_term_dicts = np.unique(term_term_stats.columnmeta['term_dict'])
    
    # calculate performance on reference examples and write to dataframe
    print('calculating performance on reference examples and writing to dataframe...')
    dataframe_path = 'benchmark_term-term_stats_dataframe.txt'
    metaheaders = ['row_dictionary', 'column_dictionary', 'year', 'datestamp', 'min_score', 'universe', 'n_prior', 'min_count', 'association_statistic', 'reference_datamatrix_path', 'row_term_dict', 'column_term_dict']
    statheaders = ['tp', 'fn', 'tn', 'fp', 'ap', 'an', 'pp', 'pn', 'n', 'auroc', 'auprc', 'tpr', 'fnr', 'tnr', 'fpr', 'ppv', 'fdr', 'npv', 'fomr', 'acc', 'mcr', 'prev', 'plr', 'nlr', 'dor', 'drr', 'darr', 'mrr', 'marr', 'f1', 'mcc', 'cos', 'fnlp', 'lrr', 'lrr_se', 'lrr_lb95', 'lrr_ub95', 'drr_lb95', 'drr_ub95', 'lor', 'lor_se', 'lor_lb95', 'lor_ub95', 'dor_lb95', 'dor_ub95', 'mi', 'nmi', 'iqr', 'min_value_association_statistic']
    with open(dataframe_path, mode='at', encoding='utf-8', errors='surrogateescape') as fw:
        writelist = metaheaders + statheaders
        fw.write('\t'.join(writelist) + '\n')
        for row_term_dict in row_term_dicts:
            row_hidxs = (term_term_stats.rowmeta['term_dict'] == row_term_dict).nonzero()[0]
            for column_term_dict in column_term_dicts:
                print('working on {0}-{1} associations...'.format(row_term_dict, column_term_dict))
                
                # get scores and labels
                print('getting scores and labels...')
                column_hidxs = (term_term_stats.columnmeta['term_dict'] == column_term_dict).nonzero()[0]
                hit = np.logical_and(np.in1d(I, row_hidxs), np.in1d(J, column_hidxs))
                Y = term_term_ref.matrix[I[hit], J[hit]]
                X = (term_term_stats.matrix[I[hit], J[hit]]).reshape(-1,1)
                X_prime = X.copy()
                if association_statistic == 'mcc':
                    X_prime = (X_prime + 1)/2
                xpmin = (X_prime[X_prime > 0]).min()/2
                xpmax = 1-(1-(X_prime[X_prime < 1]).max())/2
                X_prime[X_prime == 0] = xpmin
                X_prime[X_prime == 1] = xpmax
                logitX = np.log10(X_prime/(1-X_prime))

                # save score histograms
                print('saving score histograms...')
                values = X.reshape(-1)
                title = 'uv_{0}_rd{1}_cd{2}, pos:{3:1.3g}, neg:{4:1.3g}'.format(universe[:5], row_term_dict[:5], column_term_dict[:5], np.median(values[Y]), np.median(values[~Y]))
                save_path = '{0}/{1}_{2}_hist_{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, association_statistic, title, save_path, 'auto', (values.min(), values.max()), False)
                save_path = '{0}/{1}_{2}_zoomhist_{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, association_statistic, title, save_path, 'auto', (values.min(), values.max()), False, (np.percentile(values, 2.5), np.percentile(values, 97.5)))
                
                values = logitX.reshape(-1)
                title = 'uv_{0}_rd{1}_cd{2}, pos:{3:1.3g}, neg:{4:1.3g}'.format(universe[:5], row_term_dict[:5], column_term_dict[:5], np.median(values[Y]), np.median(values[~Y]))
                save_path = '{0}/{1}_{2}_hist_LOGIT{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'logit ' + association_statistic, title, save_path, 'auto', (values.min(), values.max()), False)
                save_path = '{0}/{1}_{2}_zoomhist_LOGIT{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'logit ' + association_statistic, title, save_path, 'auto', (values.min(), values.max()), False, (np.percentile(values, 2.5), np.percentile(values, 97.5)))
                
                # fit logistic regression classifier
                print('fitting logistic regression classifier...')
                robust_scaler = RobustScaler().fit(logitX)
                Z = robust_scaler.transform(logitX)
                logistic_regression_model = LogisticRegression(penalty='l2', C=1e3, intercept_scaling=1.0, class_weight='balanced').fit(Z, Y)
                
                if logistic_regression_model.classes_[1] == 1:
                    decision_function = logistic_regression_model.decision_function(Z)
                else:
                    decision_function = -logistic_regression_model.decision_function(Z)
                Y_pred = decision_function > 0
                min_value_association_statistic = (X.reshape(-1)[Y_pred]).min()
                
                # save decision function and predicted probability histograms
                print('saving decision function and predicted probability histograms...')
                values = decision_function.reshape(-1)
                title = 'uv_{0}_rd{1}_cd{2}, pos:{3:1.3g}, neg:{4:1.3g}'.format(universe[:5], row_term_dict[:5], column_term_dict[:5], np.median(values[Y]), np.median(values[~Y]))
                save_path = '{0}/{1}_{2}_hist_DF{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'decision fun ' + association_statistic, title, save_path, 'auto', (values.min(), values.max()), False)
                save_path = '{0}/{1}_{2}_zoomhist_DF{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'decision fun ' + association_statistic, title, save_path, 'auto', (values.min(), values.max()), False, (np.percentile(values, 2.5), np.percentile(values, 97.5)))
                
                values = (1/(1 + np.exp(-decision_function))).reshape(-1)
                title = 'uv_{0}_rd{1}_cd{2}, pos:{3:1.3g}, neg:{4:1.3g}'.format(universe[:5], row_term_dict[:5], column_term_dict[:5], np.median(values[Y]), np.median(values[~Y]))
                save_path = '{0}/{1}_{2}_hist_PP{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'pred prob ' + association_statistic, title, save_path, 'auto', (0, 1), False)
                save_path = '{0}/{1}_{2}_zoomhist_PP{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                densities, edges = plot_step_density({'positive':('-r',values[Y]), 'negative':(':b',values[~Y])}, 'pred prob ' + association_statistic, title, save_path, 'auto', (0, 1), False, (np.percentile(values, 2.5), np.percentile(values, 97.5)))
                
                # compute roc and pr curves
                print('computing roc and pr curves...')
                fpr, tpr, thresholds = roc_curve(Y, decision_function)
                precision, recall, thresholds = precision_recall_curve(Y, decision_function)
                
                auroc = roc_auc_score(Y, decision_function)
                auprc = average_precision_score(Y, decision_function)
                
                # save roc and pr curves
                print('saving roc and pr curves...')
                title = 'uv_{0}_as_{1}_rd{2}_cd{3}, auc:{4:1.3g}'.format(universe[:5], association_statistic, row_term_dict[:5], column_term_dict[:5], auprc)
                save_path = '{0}/{1}_{2}_prc_{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                fg, ax = plt.subplots(1, 1, figsize=(3,2))
                ax.plot(recall, precision, '-k', linewidth=1)
                ax.set_position([0.55/3, 0.35/2, 2.1/3, 1.3/2]) # left, bottom, width, height
                ax.set_title(title, fontsize=8)
                ax.set_ylabel('Precision', fontsize=8, labelpad=4)
                ax.set_xlabel('Recall', fontsize=8, labelpad=2)
                ax.set_ylim((0, 1))
                ax.set_xlim((0, 1))
                ax.tick_params(axis='both', which='major', bottom=True, top=False, left=True, right=False, labelbottom=True, labeltop=False, labelleft=True, labelright=False, labelsize=8)
                ax.ticklabel_format(axis='both', style='sci', scilimits=(-3,3), fontsize=8)
                ax.yaxis.offsetText.set_fontsize(8)
                ax.xaxis.offsetText.set_fontsize(8)
                fg.savefig(save_path, transparent=True, pad_inches=0, dpi=300)
                plt.close()
                
                title = 'uv_{0}_as_{1}_rd{2}_cd{3}, auc:{4:1.3g}'.format(universe[:5], association_statistic, row_term_dict[:5], column_term_dict[:5], auroc)
                save_path = '{0}/{1}_{2}_roc_{3}_yr_{4}_ds_{5}_ms_{6!s}_uv_{7}_np_{8!s}_mc_{9!s}_rd_{10}_cd_{11}.png'.format(figures_folder, row_dictionary, column_dictionary, association_statistic, year, datestamp, min_score, universe, n_prior, min_count, row_term_dict, column_term_dict)
                fg, ax = plt.subplots(1, 1, figsize=(3,2))
                ax.plot(fpr, tpr, '-k', linewidth=1)
                ax.set_position([0.55/3, 0.35/2, 2.1/3, 1.3/2]) # left, bottom, width, height
                ax.set_title(title, fontsize=8)
                ax.set_ylabel('Precision', fontsize=8, labelpad=4)
                ax.set_xlabel('Recall', fontsize=8, labelpad=2)
                ax.set_ylim((0, 1))
                ax.set_xlim((0, 1))
                ax.tick_params(axis='both', which='major', bottom=True, top=False, left=True, right=False, labelbottom=True, labeltop=False, labelleft=True, labelright=False, labelsize=8)
                ax.ticklabel_format(axis='both', style='sci', scilimits=(-3,3), fontsize=8)
                ax.yaxis.offsetText.set_fontsize(8)
                ax.xaxis.offsetText.set_fontsize(8)
                fg.savefig(save_path, transparent=True, pad_inches=0, dpi=300)
                plt.close()
                
                # save predictions for all term-term pairs
                if save_predictions:
                    print('saving predictions for all term-term pairs...')
                    predictions = {}
                    X_all = term_term_stats_all.matrix.reshape(-1,1)
                    if association_statistic == 'mcc':
                        X_all = (X_all + 1)/2
                    xamin = (X_all[X_all > 0]).min()/2
                    xamax = 1-(1-(X_all[X_all < 1]).max())/2
                    X_all[X_all == 0] = xamin
                    X_all[X_all == 1] = xamax
                    logitX_all = np.log10(X_all/(1-X_all))
                    Z_all = robust_scaler.transform(logitX_all)
                    if logistic_regression_model.classes_[1] == 1:
                        predictions['decision_function'] = logistic_regression_model.decision_function(Z_all)
                    else:
                        predictions['decision_function'] = -logistic_regression_model.decision_function(Z_all)
                    predictions['probability_positive'] = 1/(1 + np.exp(-predictions['decision_function']))
                    if not np.all(np.diff(thresholds) > 0):
                        raise ValueError('thresholds not increasing')
                    predictions['precision'] = np.interp(predictions['decision_function'], thresholds, precision[:-1])
                    predictions['recall'] = np.interp(predictions['decision_function'], thresholds, recall[:-1])
                    I0, J0 = (term_term_counts_all.matrix < min_count).nonzero()
                    IA, JA = (term_term_counts_all.matrix >= min_count).nonzero()
                    new_stats = ['{0}_dictidname'.format(row_dictionary), '{0}_dictidname'.format(column_dictionary)]
                    new_stat_mat = np.concatenate((term_term_counts_all.rowlabels[IA].reshape(-1,1), term_term_counts_all.columnlabels[JA].reshape(-1,1)), 1)
                    for stat, values in predictions.items():
                        term_term_stats_all.matrix = values.reshape(term_term_stats_all.shape[0], term_term_stats_all.shape[1])
                        term_term_stats_all.matrix[I0,J0] = 0
                        datasetIO.save_datamatrix('{0}_{1}_datamatrix_{2}_yr_{3}_ds_{4}_ms_{5!s}_uv_{6}_np_{7!s}_mc_{8!s}_as_{9}_rd_{10}_cd_{11}.txt.gz'.format(row_dictionary, column_dictionary, stat, year, datestamp, min_score, universe, n_prior, min_count, association_statistic, row_term_dict, column_term_dict), term_term_stats_all)
                        datasetIO.save_datamatrix('{0}_{1}_datamatrix_{2}_yr_{3}_ds_{4}_ms_{5!s}_uv_{6}_np_{7!s}_mc_{8!s}_as_{9}_rd_{10}_cd_{11}.pickle'.format(row_dictionary, column_dictionary, stat, year, datestamp, min_score, universe, n_prior, min_count, association_statistic, row_term_dict, column_term_dict), term_term_stats_all)
                        new_stats.append(stat)
                        new_stat_mat = np.append(new_stat_mat, (term_term_stats_all.matrix[IA,JA]).reshape(-1,1), 1)
                    new_df = pd.DataFrame(data=new_stat_mat, columns=new_stats)
                    dataframe_path = '{0}_{1}_dataframe_yr_{2}_ds_{3}_ms_{4!s}_uv_{5}_np_{6!s}_mc_{7!s}.txt.gz'.format(row_dictionary, column_dictionary, year, datestamp, min_score, universe, n_prior, min_count)
                    joined_dataframe_path = '{0}_{1}_dataframe_yr_{2}_ds_{3}_ms_{4!s}_uv_{5}_np_{6!s}_mc_{7!s}_as_{8}_rd_{9}_cd_{10}.txt.gz'.format(row_dictionary, column_dictionary, year, datestamp, min_score, universe, n_prior, min_count, association_statistic, row_term_dict, column_term_dict)
                    df = pd.read_table(dataframe_path, compression='gzip', index_col=False)
                    joined_df = df.set_index(new_stats[:2]).join(new_df.set_index(new_stats[:2]))
                    joined_df.sort_values(by=association_statistic, ascending=False, inplace=True)
                    joined_df.to_csv(joined_dataframe_path, sep='\t', compression='gzip')

                # compute classifier performance statistics
                # note, these are in-sample statistics
                # we are not worried about overfitting
                # because we only have one feature
                # and we are not trying to build a rigorous ML model
                # we are simply trying to answer the question,
                # given a reference set of positive and negative examples,
                # which association statistic ranks term-term pairs the best?
                print('computing classifier performance statistics...')
                tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()
                
                # incorporate a random prior with effective sample size = n_prior
                prevalence = (tp + fn)/(tn + fp + fn + tp)
                tp += n_prior*prevalence/2
                fn += n_prior*prevalence/2
                tn += n_prior*(1-prevalence)/2
                fp += n_prior*(1-prevalence)/2
                
                ap = tp + fn
                an = fp + tn
                pp = tp + fp
                pn = tn + fn
                n = tn + fp + fn + tp
                
                tpr = tp/ap # sensitivity, recall
                fnr = fn/ap # 1-tpr, 1-sensitivity, 1-recall
                tnr = tn/an # specificity
                fpr = fp/an # 1-tnr, 1-specificity
                
                ppv = tp/pp # precision
                fdr = fp/pp # 1-ppv, 1-precision
                npv = tn/pn
                fomr = fn/pn # 1-npv
                
                acc = (tp + tn)/n
                mcr = (fp + fn)/n # 1-acc
                prev = ap/n
                
                plr = (tp/fp)/(ap/an) # tpr/fpr, sensitivity/(1-specificity), ratio of positives to negatives in positive predictions relative to ratio in whole sample, higher is better
                nlr = (fn/tn)/(ap/an) # fnr/tnr, (1-sensitivity)/specificity, ratio of positives to negatives in negative predictions relative to ratio in whole sample, lower is better
                dor = (tp/fp)/(fn/tn) # plr/nlr, ratio of positives to negatives in positive predictions, divided by ratio of positives to negatives in negative predictions
                drr = (tp/pp)/(fn/pn) # ppv/fomr, relative risk or risk ratio, fraction of positives in positive predictions divided by fraction of positives in negative predictions
                darr = (tp/pp) - (fn/pn) # ppv - fomr, absolute risk reduction, fraction of positives in positive predictions minus fraction of positives in negative predictions
                mrr = (tp/pp)/(ap/n) # ppv/prev, modified (by me) relative risk or risk ratio, fraction of positives in positive predictions divided by fraction of positives in whole sample
                marr = (tp/pp) - (ap/n) # ppv - prev, modified (by me) absolute risk reduction, fraction of positives in positive predictions minus fraction of positives in whole sample
                
                f1 = (1 + (1**2))*ppv*tpr/((1**2)*ppv + tpr)
                mcc = (tp*tn - fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))
                cos = tp/np.sqrt((tp+fp)*(tp+fn)) # ochiai
                fnlp = -hypergeom.logsf(tp, n, ap, pp, loc=1)/np.log(10)
                
                lrr = np.log10(tp) - np.log10(tp+fp) - np.log10(fn) + np.log10(fn+tn) # log10 of relative risk
                lrr_se = np.sqrt(fp/tp/(tp+fp) + tn/fn/(fn+tn))/np.log(10) # standard error of log10 of relative risk
                lrr_lb95 = lrr - 1.96*lrr_se
                lrr_ub95 = lrr + 1.96*lrr_se
                drr_lb95 = 10**lrr_lb95
                drr_ub95 = 10**lrr_ub95
                
                lor = np.log10(tp) - np.log10(fp) - np.log10(fn) + np.log10(tn) # log10 of odds ratio
                lor_se = np.sqrt(1/tp + 1/fp + 1/fn + 1/tn)/np.log(10) # standard error of log10 of odds ratio
                lor_lb95 = lor - 1.96*lor_se
                lor_ub95 = lor + 1.96*lor_se
                dor_lb95 = 10**lor_lb95
                dor_ub95 = 10**lor_ub95
                
                mi, nmi, iqr = mutualinformation(tp, fp, fn, tn) # mutual information, normalized mutual information, information quality ratio
                
                # write to dataframe
                print('writing to dataframe...')
                count_stats = [tp, fn, tn, fp, ap, an, pp, pn, n]
                other_stats = [auroc, auprc, tpr, fnr, tnr, fpr, ppv, fdr, npv, fomr, acc, mcr, prev, plr, nlr, dor, drr, darr, mrr, marr, f1, mcc, cos, fnlp, lrr, lrr_se, lrr_lb95, lrr_ub95, drr_lb95, drr_ub95, lor, lor_se, lor_lb95, lor_ub95, dor_lb95, dor_ub95, mi, nmi, iqr, min_value_association_statistic]
                
                writelist = [row_dictionary, column_dictionary, year, datestamp, str(min_score), universe, str(n_prior), str(min_count), association_statistic, reference_datamatrix_path, row_term_dict, column_term_dict]
                writelist += [str(s) for s in count_stats]
                writelist += ['{0:1.5g}'.format(s) for s in other_stats]
                fw.write('\t'.join(writelist) + '\n')

    print('done benchmark_term-term_stats_from_termite.py')
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Benchmark term-term association statistics from termite.')
    parser.add_argument('--dictionaries', help='two dictionaries of biomedical terms, first for row terms and second for column terms', type=str, nargs='+')
    parser.add_argument('--year', help='year of termite file', type=str, default='all')
    parser.add_argument('--datestamp', help='datestamp of termite file', type=str, default='all')
    parser.add_argument('--min_score', help='min score for filtering term-pmid associations', type=int, default=2)
    parser.add_argument('--universe', help='the full set of publications considered in the analysis (intersectionunion, union, medline, or infinity)', type=str, default='intersectionunion')
    parser.add_argument('--n_prior', help='prior effective sample size', type=int, default=1)
    parser.add_argument('--min_count', help='min term-term count for computing association statistics', type=int, default=5)
    parser.add_argument('--association_statistic', help='association statistic derived from term-term counts', type=str, default='mcc')
    parser.add_argument('--reference_datamatrix_path', help='path to datamatrix of positive and negative term-term pairs', type=str)
    parser.add_argument('--save_predictions', help='whether or not to save predictions of classifier', type=bool, default=False)
    args = parser.parse_args()
    main(args.dictionaries, args.year, args.datestamp, args.min_score, args.universe, args.n_prior, args.min_count, args.association_statistic, args.reference_datamatrix_path, args.save_predictions)
